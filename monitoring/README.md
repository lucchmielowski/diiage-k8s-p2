# Surveillance Kubernetes avec OpenTelemetry

Ce r√©pertoire contient une stack de surveillance compl√®te pour enseigner l'observabilit√© Kubernetes avec OpenTelemetry, incluant l'injection automatique de sidecars, le tra√ßage distribu√© et la collecte de m√©triques.

## üìö Table des mati√®res

- [Connaissances pr√©requises](#connaissances-pr√©requises)
- [Vue d'ensemble de l'architecture](#vue-densemble-de-larchitecture)
- [Composants](#composants)
- [Pr√©requis](#pr√©requis)
- [Guide d'installation](#guide-dinstallation)
- [Utilisation de la stack](#utilisation-de-la-stack)
- [Exercices pour √©tudiants](#exercices-pour-√©tudiants)
- [D√©pannage](#d√©pannage)

## üìñ Connaissances pr√©requises

L'observabilit√© moderne repose sur trois piliers compl√©mentaires qui travaillent ensemble pour fournir une vue compl√®te de la sant√© et du comportement de votre syst√®me. Chaque pilier sert un objectif sp√©cifique et r√©pond √† diff√©rentes questions sur votre syst√®me.

### M√©triques

Les m√©triques sont des mesures quantitatives du comportement du syst√®me collect√©es au fil du temps. Elles fournissent des points de donn√©es num√©riques qui peuvent √™tre agr√©g√©s, analys√©s et visualis√©s pour comprendre les tendances et les mod√®les. Les exemples incluent l'utilisation du CPU, la consommation de m√©moire, les taux de requ√™tes, les compteurs d'erreurs et les temps de r√©ponse.

**Ce qu'elles vous disent :** Les m√©triques r√©pondent aux questions "quoi" et "quand" concernant la sant√© de votre syst√®me. Elles vous montrent que quelque chose se produit (par exemple, utilisation √©lev√©e du CPU, taux d'erreur accru) et quand cela se produit.

**Cas d'usage :**
- Surveillance de la sant√© du syst√®me et des tendances de performance
- Configuration d'alertes bas√©es sur des seuils
- Planification de la capacit√© et optimisation des ressources
- Cr√©ation de tableaux de bord pour la surveillance en temps r√©el

**Outils dans cette stack :** Prometheus collecte et stocke les m√©triques, Grafana les visualise.

### Logs

Les logs sont des enregistrements discrets d'√©v√©nements qui se sont produits dans votre syst√®me √† des moments pr√©cis. Chaque entr√©e de log inclut g√©n√©ralement un horodatage, un niveau de s√©v√©rit√© et des informations contextuelles d√©taill√©es sur ce qui s'est pass√©. Les logs capturent l'histoire de l'ex√©cution de votre application.

**Ce qu'ils vous disent :** Les logs r√©pondent √† la question "que s'est-il pass√©". Ils fournissent un contexte d√©taill√© sur des √©v√©nements sp√©cifiques, des erreurs et des changements d'√©tat dans votre application.

**Cas d'usage :**
- D√©bogage des erreurs et exceptions d'application
- Audit des actions utilisateur et des changements syst√®me
- Compr√©hension de la s√©quence d'√©v√©nements menant √† un probl√®me
- Conformit√© et surveillance de la s√©curit√©

**Outils dans cette stack :** OpenTelemetry Collector peut recevoir des logs ; en production, vous ajouteriez typiquement Loki ou Elasticsearch pour l'agr√©gation et la recherche de logs.

### Traces

Les traces suivent le parcours complet d'une requ√™te lorsqu'elle traverse votre syst√®me distribu√©. Une trace consiste en plusieurs spans, o√π chaque span repr√©sente une unit√© de travail (comme un appel de fonction ou une communication service-√†-service). Les traces montrent les relations entre diff√©rents composants et combien de temps chaque √©tape a pris.

**Ce qu'elles vous disent :** Les traces r√©pondent aux questions "o√π" et "pourquoi" concernant les probl√®mes de performance. Elles r√©v√®lent quel service ou composant cause des ralentissements et montrent le chemin complet qu'une requ√™te prend √† travers votre architecture de microservices.

**Cas d'usage :**
- Identification des goulots d'√©tranglement de performance dans les syst√®mes distribu√©s
- Compr√©hension des d√©pendances de services et des mod√®les de communication
- D√©bogage de probl√®mes qui s'√©tendent sur plusieurs services
- Optimisation des flux de requ√™tes et r√©duction de la latence

**Outils dans cette stack :** Tempo stocke et interroge les traces, Grafana les visualise avec des graphes de services et des chronologies de traces.

### Pourquoi utiliser les trois ensemble ?

Utiliser les m√©triques, les logs et les traces ensemble cr√©e une strat√©gie d'observabilit√© puissante :

1. **Les m√©triques** vous alertent qu'il y a un probl√®me (par exemple, taux d'erreur √©lev√©)
2. **Les logs** fournissent le contexte sur ce qui s'est mal pass√© (par exemple, messages d'erreur sp√©cifiques)
3. **Les traces** vous aident √† identifier pr√©cis√©ment o√π dans votre syst√®me distribu√© le probl√®me a pris naissance (par exemple, quel service est lent)

Cette approche holistique est essentielle pour comprendre et d√©boguer les architectures de microservices complexes dans Kubernetes, o√π une seule requ√™te utilisateur peut toucher des dizaines de services.

## üèóÔ∏è Vue d'ensemble de l'architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Kubernetes Cluster                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      Auto-Instrumentation             ‚îÇ
‚îÇ  ‚îÇ  Your App Pod    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                             ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ OTel Sidecar     ‚îÇ  Injected by                ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (auto-injected)  ‚îÇ  OpenTelemetry Operator     ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ         ‚îÇ
‚îÇ           ‚îÇ OTLP (traces, metrics, logs)          ‚îÇ         ‚îÇ
‚îÇ           ‚îÇ                                       ‚îÇ         ‚îÇ
‚îÇ           ‚ñº                                       ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   OpenTelemetry Collector           ‚îÇ          ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   - Receives: OTLP (gRPC/HTTP)      ‚îÇ          ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   - Processes: Batch, Filter        ‚îÇ          ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   - Exports: Tempo, Prometheus      ‚îÇ          ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ         ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                      ‚îÇ         ‚îÇ
‚îÇ         ‚îÇ Traces           ‚îÇ Metrics              ‚îÇ         ‚îÇ
‚îÇ         ‚ñº                  ‚ñº                      ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ    Tempo     ‚îÇ   ‚îÇ  Prometheus  ‚îÇ              ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  (Tracing)   ‚îÇ   ‚îÇ  (Metrics)   ‚îÇ              ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ         ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                      ‚îÇ         ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ         ‚îÇ
‚îÇ                  ‚îÇ                                ‚îÇ         ‚îÇ
‚îÇ                  ‚ñº                                ‚îÇ         ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ         ‚îÇ
‚îÇ         ‚îÇ    Grafana      ‚îÇ                       ‚îÇ         ‚îÇ
‚îÇ         ‚îÇ (Visualization) ‚îÇ                       ‚îÇ         ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ         ‚îÇ
‚îÇ                                                   ‚îÇ         ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ         ‚îÇ                                                   ‚îÇ
‚îÇ         ‚ñº                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ  OpenTelemetry Operator          ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ  - Manages OTel Collector CRD    ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ  - Auto-instrumentation injection‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üß© Composants

### 1. **OpenTelemetry Operator**
- G√®re les d√©ploiements d'OpenTelemetry Collector
- Injecte automatiquement des sidecars d'instrumentation dans les pods
- Prend en charge l'auto-instrumentation Python, Java, Node.js, .NET

### 2. **OpenTelemetry Collector**
- Re√ßoit les donn√©es de t√©l√©m√©trie via OTLP (gRPC et HTTP)
- Traite et met en lot les donn√©es
- Exporte les traces vers Tempo et les m√©triques vers Prometheus

### 3. **Tempo**
- Backend de tra√ßage distribu√©
- Stocke et interroge les traces
- Int√©gr√© avec Grafana pour la visualisation

### 4. **Prometheus**
- Base de donn√©es de m√©triques de s√©ries temporelles
- Scrape les m√©triques depuis les applications et Kubernetes
- Re√ßoit les m√©triques depuis OpenTelemetry Collector
- **Note :** Cette stack utilise Prometheus standalone. Le Prometheus Operator (voir [Alternative dans l'Exercice 7](#exercice-7--configurer-des-alertes-avanc√©)) est une alternative plus avanc√©e qui simplifie la gestion des alertes et la d√©couverte de services via des CRDs.

### 5. **Grafana**
- Plateforme de visualisation unifi√©e
- Pr√©-configur√©e avec les sources de donn√©es Prometheus et Tempo
- Inclut des tableaux de bord d'exemple

## ‚úÖ Pr√©requis

- Cluster Kubernetes (v1.24+)
- `kubectl` configur√© pour acc√©der √† votre cluster
- `helm` (v3.0+) install√©
- Compr√©hension de base des concepts Kubernetes et Helm

## üì• Guide d'installation

### Option 1 : Installation automatis√©e (Recommand√©e)

Le moyen le plus simple d'installer toute la stack de surveillance :

```bash
cd monitoring
chmod +x install.sh
./install.sh
```

Ce script va :
1. Ajouter les d√©p√¥ts Helm requis
2. Installer cert-manager (pour les webhooks OpenTelemetry Operator)
3. Installer OpenTelemetry Operator
4. D√©ployer tous les composants de surveillance via Helm (Tempo, Prometheus, Grafana, OpenTelemetry Collector)
5. Cr√©er la ressource Instrumentation

**Apr√®s l'installation**, acc√©dez √† Grafana :

```bash
kubectl port-forward -n monitoring svc/grafana 3000:3000
```

Ouvrez http://localhost:3000 (admin/admin)

---

### Option 2 : Installation manuelle avec Helm

#### √âtape 1 : Ajouter les d√©p√¥ts Helm

```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update
```

#### √âtape 2 : Installer cert-manager

cert-manager est requis pour les webhooks OpenTelemetry Operator.

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.yaml

# Attendre que cert-manager soit pr√™t
kubectl wait --for=condition=available --timeout=300s \
  deployment/cert-manager \
  deployment/cert-manager-webhook \
  deployment/cert-manager-cainjector \
  -n cert-manager
```

#### √âtape 3 : Installer OpenTelemetry Operator

```bash
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/download/v0.91.0/opentelemetry-operator.yaml

# Attendre que l'op√©rateur soit pr√™t
kubectl wait --for=condition=available --timeout=300s \
  deployment/opentelemetry-operator-controller-manager \
  -n opentelemetry-operator-system
```

#### √âtape 4 : Cr√©er le namespace monitoring

```bash
kubectl apply -f namespace.yaml
```

#### √âtape 5 : D√©ployer la stack de surveillance via Helm

**Installer Tempo :**

```bash
helm upgrade --install tempo grafana/tempo \
  --namespace monitoring \
  --values tempo/values.yaml \
  --wait
```

**Installer Prometheus :**

```bash
helm upgrade --install prometheus prometheus-community/prometheus \
  --namespace monitoring \
  --values prometheus/values.yaml \
  --wait
```

**Installer Grafana :**

```bash
helm upgrade --install grafana grafana/grafana \
  --namespace monitoring \
  --values grafana/values.yaml \
  --wait
```

**Installer OpenTelemetry Collector :**

```bash
helm upgrade --install otel-collector open-telemetry/opentelemetry-collector \
  --namespace monitoring \
  --values opentelemetry-collector/values.yaml \
  --wait
```

#### √âtape 6 : Cr√©er la ressource Instrumentation

Cette ressource d√©finit comment les applications doivent √™tre auto-instrument√©es.

```bash
kubectl apply -f demo-instrumented/instrumentation.yaml
```

#### √âtape 7 : D√©ployer les applications de d√©monstration (Optionnel)

D√©ployer des applications d'exemple avec auto-instrumentation :

```bash
kubectl apply -f demo-instrumented/demo-app-instrumented.yaml
```

Cela d√©ploie trois applications de d√©monstration :
- **demo-python-app** : Serveur HTTP Python avec auto-instrumentation
- **demo-nodejs-app** : Serveur HTTP Node.js avec auto-instrumentation
- **demo-java-app** : Application Spring Boot avec auto-instrumentation

#### √âtape 8 : Acc√©der √† la stack

**Port-forward Grafana :**

```bash
kubectl port-forward -n monitoring svc/grafana 3000:3000
```

Acc√©dez √† Grafana √† : http://localhost:3000
- Nom d'utilisateur : `admin`
- Mot de passe : `admin`

**Port-forward Prometheus (optionnel) :**

```bash
kubectl port-forward -n monitoring svc/prometheus-server 9090:80
```

Acc√©dez √† Prometheus √† : http://localhost:9090

---

### Personnalisation des d√©ploiements Helm

Tous les fichiers de valeurs Helm sont situ√©s dans leurs r√©pertoires de composants respectifs :
- `grafana/values.yaml` - Configuration Grafana
- `prometheus/values.yaml` - Configuration Prometheus
- `tempo/values.yaml` - Configuration Tempo
- `opentelemetry-collector/values.yaml` - Configuration Collector

Vous pouvez personnaliser ces fichiers pour ajuster :
- Les limites et demandes de ressources
- La persistance du stockage
- Les politiques de r√©tention
- Les intervalles de scraping
- Les configurations de sources de donn√©es

## üéØ Utilisation de la stack

### Comment activer l'auto-instrumentation

Pour activer l'auto-instrumentation pour vos applications, ajoutez des annotations √† votre sp√©cification Pod :

#### Application Python

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-python-app
spec:
  template:
    metadata:
      annotations:
        instrumentation.opentelemetry.io/inject-python: "monitoring/demo-instrumentation"
    spec:
      containers:
      - name: app
        image: my-python-app:latest
        env:
        - name: OTEL_SERVICE_NAME
          value: my-python-app
```

#### Application Node.js

```yaml
annotations:
  instrumentation.opentelemetry.io/inject-nodejs: "monitoring/demo-instrumentation"
```

#### Application Java

```yaml
annotations:
  instrumentation.opentelemetry.io/inject-java: "monitoring/demo-instrumentation"
```

#### Application .NET

```yaml
annotations:
  instrumentation.opentelemetry.io/inject-dotnet: "monitoring/demo-instrumentation"
```

### Activer le scraping Prometheus

Ajoutez ces annotations pour permettre √† Prometheus de scraper les m√©triques depuis vos pods :

```yaml
annotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"
```

### Visualiser les traces dans Grafana

1. Ouvrez Grafana (http://localhost:3000)
2. Naviguez vers **Explore** (ic√¥ne boussole)
3. S√©lectionnez **Tempo** comme source de donn√©es
4. Utilisez l'onglet **Search** pour trouver les traces
5. Filtrez par nom de service, op√©ration, tags, etc.

### Visualiser les m√©triques dans Grafana

1. Dans Grafana, naviguez vers **Explore**
2. S√©lectionnez **Prometheus** comme source de donn√©es
3. Utilisez des requ√™tes PromQL, par exemple :
   - `rate(http_requests_total[5m])` - Taux de requ√™tes HTTP
   - `otelcol_receiver_accepted_spans` - Spans re√ßus par le collector
   - `up` - Disponibilit√© du service

## üéì Exercices pour √©tudiants

### Exercice 1 : V√©rifier l'installation

**Objectif :** S'assurer que tous les composants fonctionnent correctement.

**T√¢ches :**
1. Lister tous les pods dans le namespace `monitoring`
2. V√©rifier que tous les d√©ploiements sont pr√™ts
3. V√©rifier que l'OpenTelemetry Collector re√ßoit des donn√©es

**Commandes :**
```bash
kubectl get pods -n monitoring
kubectl get deployments -n monitoring
kubectl logs -n monitoring deployment/otel-collector-collector -f
```

**R√©sultats attendus :**
- Tous les pods devraient √™tre dans l'√©tat `Running`
- Tous les d√©ploiements devraient afficher `READY 1/1`
- Les logs du collector ne devraient montrer aucune erreur

---

### Exercice 2 : D√©ployer une application instrument√©e

**Objectif :** D√©ployer votre premi√®re application auto-instrument√©e.

**T√¢ches :**
1. D√©ployer l'application Python de d√©monstration
2. V√©rifier que le sidecar a √©t√© inject√©
3. G√©n√©rer du trafic
4. Trouver les traces dans Grafana

**Commandes :**
```bash
# D√©ployer
kubectl apply -f demo-instrumented/demo-app-instrumented.yaml

# V√©rifier si le sidecar a √©t√© inject√©
kubectl get pod -n monitoring -l app=demo-python-app -o yaml | grep -A 5 "initContainers"

# G√©n√©rer du trafic
kubectl port-forward -n monitoring svc/demo-python-app 8080:8080
# Dans un autre terminal :
for i in {1..20}; do curl http://localhost:8080; sleep 1; done

# Acc√©der √† Grafana
kubectl port-forward -n monitoring svc/grafana 3000:3000
# Ouvrir http://localhost:3000 et explorer les traces
```

**Questions :**
- Combien de conteneurs y a-t-il dans le pod apr√®s l'injection ?
- Quelles traces voyez-vous dans Tempo ?
- Quelles m√©triques apparaissent dans Prometheus ?

---

### Exercice 3 : Ajouter l'instrumentation aux applications existantes

**Objectif :** Ajouter l'auto-instrumentation √† l'application demo-frontend existante.

**T√¢ches :**
1. Copier le chart Helm `demo-frontend`
2. Ajouter l'annotation d'instrumentation
3. D√©ployer et v√©rifier que les traces apparaissent

**Indice :** Ajoutez ceci aux m√©tadonn√©es du template de d√©ploiement :
```yaml
annotations:
  instrumentation.opentelemetry.io/inject-python: "monitoring/demo-instrumentation"
```

---

### Exercice 4 : Cr√©er un tableau de bord personnalis√©

**Objectif :** Construire un tableau de bord Grafana pour votre application.

**T√¢ches :**
1. Dans Grafana, cr√©er un nouveau tableau de bord
2. Ajouter un panneau affichant le taux de requ√™tes
3. Ajouter un panneau affichant le taux d'erreurs
4. Ajouter un panneau affichant la dur√©e des requ√™tes (p95, p99)

**Exemples de requ√™tes PromQL :**
```promql
# Taux de requ√™tes
rate(http_server_requests_total[5m])

# Taux d'erreurs
rate(http_server_requests_total{status=~"5.."}[5m])

# Dur√©e des requ√™tes p95
histogram_quantile(0.95, rate(http_server_duration_bucket[5m]))
```

---

### Exercice 5 : Tracer une requ√™te distribu√©e

**Objectif :** Comprendre le tra√ßage distribu√© √† travers les services.

**T√¢ches :**
1. D√©ployer √† la fois `demo-frontend` et `demo-backend` avec instrumentation
2. Faire une requ√™te qui va frontend ‚Üí backend
3. Trouver la trace distribu√©e dans Tempo
4. Analyser les spans de la trace

**Questions :**
- Combien de spans y a-t-il dans la trace ?
- Quelle est la dur√©e totale de la requ√™te ?
- O√π la plupart du temps est-il pass√© ?

---

### Exercice 6 : Investiguer un probl√®me de performance

**Objectif :** Utiliser les outils d'observabilit√© pour d√©boguer un service lent.

**Sc√©nario :** Un de vos services r√©pond lentement.

**T√¢ches :**
1. Trouver les traces lentes dans Tempo (dur√©e > 1s)
2. Identifier quel span prend le plus de temps
3. Corr√©ler avec les m√©triques dans Prometheus
4. Proposer une solution

---

### Exercice 7 : Configurer des alertes (Avanc√©)

**Objectif :** Cr√©er une alerte Prometheus pour des taux d'erreur √©lev√©s.

**T√¢ches :**
1. Cr√©er un ConfigMap avec des r√®gles d'alerte Prometheus
2. Configurer Prometheus pour charger ces r√®gles
3. D√©finir une alerte pour un taux d'erreur > 5%
4. Tester l'alerte en g√©n√©rant des erreurs

**Note :** Cette stack utilise Prometheus standalone (pas le Prometheus Operator), donc les alertes sont configur√©es via des fichiers de r√®gles plut√¥t que via la ressource `PrometheusRule` CRD.

**√âtape 1 : Cr√©er un ConfigMap avec les r√®gles d'alerte**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: monitoring
data:
  alerts.yml: |
    groups:
    - name: app
      rules:
      - alert: HighErrorRate
        expr: rate(http_server_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for {{ $labels.service }}"
```

**√âtape 2 : Mettre √† jour la configuration Prometheus**

√âditer `prometheus/values.yaml` pour ajouter la configuration des r√®gles d'alerte :

```yaml
serverFiles:
  prometheus.yml:
    scrape_configs:
      # ... (configuration existante)
    
    # Ajouter la configuration des r√®gles d'alerte
    rule_files:
      - /etc/prometheus/rules/*.yml

# Ajouter un volume pour monter le ConfigMap
server:
  extraVolumes:
    - name: alert-rules
      configMap:
        name: prometheus-alerts
  extraVolumeMounts:
    - name: alert-rules
      mountPath: /etc/prometheus/rules
```

**√âtape 3 : Appliquer les changements**

```bash
# Cr√©er le ConfigMap
kubectl apply -f prometheus-alerts-configmap.yaml

# Mettre √† jour Prometheus
helm upgrade prometheus prometheus-community/prometheus \
  --namespace monitoring \
  --values prometheus/values.yaml
```

**√âtape 4 : V√©rifier les alertes**

Acc√©der √† Prometheus et naviguer vers **Alerts** pour voir les alertes configur√©es.

---

#### Alternative : Utiliser Prometheus Operator (Plus simple)

Si vous pr√©f√©rez une approche plus simple et plus native √† Kubernetes pour g√©rer les alertes, vous pouvez utiliser le **Prometheus Operator** au lieu de Prometheus standalone. Le Prometheus Operator fournit des Custom Resource Definitions (CRDs) qui simplifient la gestion de Prometheus et de ses r√®gles d'alerte.

**Avantages du Prometheus Operator :**
- ‚úÖ Gestion des alertes via des ressources Kubernetes (`PrometheusRule` CRD)
- ‚úÖ D√©couverte automatique des services √† scraper via `ServiceMonitor`
- ‚úÖ Configuration d√©clarative via des ressources Kubernetes
- ‚úÖ Int√©gration native avec AlertManager
- ‚úÖ Gestion simplifi√©e des mises √† jour et de la configuration

**Pour utiliser Prometheus Operator :**

1. **Remplacer le chart Prometheus** dans `install.sh` :
   ```bash
   # Au lieu de prometheus-community/prometheus
   helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
     --namespace monitoring \
     --values prometheus-operator/values.yaml
   ```

2. **Cr√©er des alertes avec PrometheusRule** (beaucoup plus simple) :
   ```yaml
   apiVersion: monitoring.coreos.com/v1
   kind: PrometheusRule
   metadata:
     name: app-alerts
     namespace: monitoring
   spec:
     groups:
     - name: app
       rules:
       - alert: HighErrorRate
         expr: rate(http_server_requests_total{status=~"5.."}[5m]) > 0.05
         for: 5m
         annotations:
           summary: "High error rate detected"
           description: "Error rate is above 5% for {{ $labels.service }}"
   ```

3. **D√©couvrir automatiquement les services** avec `ServiceMonitor` :
   ```yaml
   apiVersion: monitoring.coreos.com/v1
   kind: ServiceMonitor
   metadata:
     name: my-app
     namespace: monitoring
   spec:
     selector:
       matchLabels:
         app: my-app
     endpoints:
     - port: http
       path: /metrics
   ```

**Note :** Cette stack utilise actuellement Prometheus standalone pour rester simple et l√©ger. Le Prometheus Operator est recommand√© pour les environnements de production o√π vous avez besoin de plus de fonctionnalit√©s et d'une gestion plus d√©clarative.

---

### Exercice 8 : Personnaliser l'instrumentation

**Objectif :** Modifier la ressource Instrumentation pour changer l'√©chantillonnage.

**T√¢ches :**
1. √âditer le `instrumentation.yaml`
2. Changer le sampler de `always_on` √† `traceidratio`
3. D√©finir le taux d'√©chantillonnage √† 50%
4. Appliquer et observer la diff√©rence

**Indice :**
```yaml
sampler:
  type: traceidratio
  argument: "0.5"
```

## üêõ D√©pannage

### Probl√®mes de release Helm

**Lister toutes les releases Helm :**
```bash
helm list -n monitoring
```

**V√©rifier le statut d'une release Helm :**
```bash
helm status <release-name> -n monitoring
# Exemples : grafana, prometheus, tempo, otel-collector
```

**Obtenir les valeurs d'une release Helm :**
```bash
helm get values <release-name> -n monitoring
```

**Restaurer une mise √† jour √©chou√©e :**
```bash
helm rollback <release-name> -n monitoring
```

**D√©sinstaller et r√©installer :**
```bash
helm uninstall <release-name> -n monitoring
helm upgrade --install <release-name> <chart> --namespace monitoring --values <values-file> --wait
```

### Pods ne d√©marrent pas

**V√©rifier le statut du pod :**
```bash
kubectl describe pod <pod-name> -n monitoring
kubectl logs <pod-name> -n monitoring
```

**V√©rifier le statut du d√©ploiement Helm :**
```bash
kubectl get deployments -n monitoring
helm status grafana -n monitoring
helm status prometheus -n monitoring
helm status tempo -n monitoring
helm status otel-collector -n monitoring
```

### Aucune trace n'appara√Æt

**V√©rifier les logs du collector :**
```bash
# Note : Le nom du d√©ploiement Helm peut diff√©rer
kubectl logs -n monitoring deployment/otel-collector-opentelemetry-collector -f
```

**V√©rifier l'instrumentation :**
```bash
kubectl get instrumentation -n monitoring
kubectl describe pod <app-pod> -n monitoring
```

**Probl√®mes courants :**
- Le format de l'annotation est incorrect (doit √™tre `namespace/instrumentation-name`)
- Le langage de l'application n'est pas pris en charge pour l'auto-instrumentation
- Probl√®mes de connectivit√© r√©seau vers le collector
- Les noms de services ont chang√© avec Helm (par exemple, `prometheus-server` au lieu de `prometheus`)

### Grafana n'affiche pas de donn√©es

**V√©rifier la configuration de la source de donn√©es :**
1. Grafana ‚Üí Configuration ‚Üí Data Sources
2. Tester la connexion √† Prometheus et Tempo
3. V√©rifier que les URLs sont correctes (notez les noms de services Helm) :
   - Prometheus : `http://prometheus-server.monitoring.svc.cluster.local:80`
   - Tempo : `http://tempo.monitoring.svc.cluster.local:3200`

**Reconfigurer les sources de donn√©es via Helm :**
```bash
# √âditer la section datasources de grafana/values.yaml
# Puis mettre √† jour la release
helm upgrade grafana grafana/grafana -n monitoring --values grafana/values.yaml
```

### Utilisation √©lev√©e des ressources

**R√©duire la r√©tention dans Prometheus :**

√âditer `prometheus/values.yaml` :
```yaml
server:
  retention: "1d"  # Au lieu de 7d
```

Appliquer les changements :
```bash
helm upgrade prometheus prometheus-community/prometheus -n monitoring --values prometheus/values.yaml
```

**R√©duire la r√©tention dans Tempo :**

√âditer `tempo/values.yaml` :
```yaml
tempo:
  config: |
    compactor:
      compaction:
        block_retention: 24h  # Au lieu de 48h
```

Appliquer les changements :
```bash
helm upgrade tempo grafana/tempo -n monitoring --values tempo/values.yaml
```

**Ajuster l'√©chantillonnage :**

√âditer `demo-instrumented/instrumentation.yaml` :
```yaml
sampler:
  type: traceidratio
  argument: "0.1"  # √âchantillonner seulement 10% des traces
```

Appliquer les changements :
```bash
kubectl apply -f demo-instrumented/instrumentation.yaml
```

### Changements de configuration non appliqu√©s

Si vous modifiez un fichier de valeurs et que les changements n'apparaissent pas :

```bash
# Mettre √† jour la release Helm avec les nouvelles valeurs
helm upgrade <release-name> <chart> -n monitoring --values <values-file>

# Forcer la recr√©ation des pods
helm upgrade <release-name> <chart> -n monitoring --values <values-file> --force

# V√©rifier la nouvelle configuration
helm get values <release-name> -n monitoring
```

## üìñ Ressources suppl√©mentaires

### Documentation OpenTelemetry
- [OpenTelemetry Operator](https://github.com/open-telemetry/opentelemetry-operator)
- [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/)
- [Auto-instrumentation](https://opentelemetry.io/docs/instrumentation/)

### Outils d'observabilit√©
- [Prometheus](https://prometheus.io/docs/)
- [Grafana Tempo](https://grafana.com/docs/tempo/latest/)
- [Grafana](https://grafana.com/docs/grafana/latest/)

### Ressources PromQL
- [PromQL Basics](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Query Examples](https://prometheus.io/docs/prometheus/latest/querying/examples/)

## üöÄ Prochaines √©tapes

- Int√©grer avec Loki pour l'agr√©gation de logs
- Ajouter l'alerting avec AlertManager
- Explorer l'int√©gration avec un service mesh (Istio/Linkerd)
- Configurer le stockage √† long terme (S3, GCS)
- Impl√©menter les SLOs et budgets d'erreur
