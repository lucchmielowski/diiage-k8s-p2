# Resiliency Principles Guide

## Table of Contents
- [Introduction](#introduction)
- [Golden Signals](#golden-signals)
- [SLIs, SLOs, and SLAs](#slis-slos-and-slas)
- [Error Budgets](#error-budgets)
- [Chaos Engineering](#chaos-engineering)
- [Circuit Breakers and Rate Limiting](#circuit-breakers-and-rate-limiting)
- [Retry Strategies](#retry-strategies)
- [Graceful Degradation](#graceful-degradation)
- [Observability](#observability)
- [Deployment Strategies](#deployment-strategies)
- [Practical Patterns](#practical-patterns)

## Introduction

La rÃ©silience systÃ¨me, c'est la capacitÃ© d'un systÃ¨me Ã  continuer de fonctionner malgrÃ© les pannes, les pics de charge, ou les comportements inattendus. C'est pas juste Ã©viter les pannes - c'est savoir comment rÃ©agir quand elles arrivent.

### Principe de base

```
Pannes inÃ©vitables â†’ Design pour la panne â†’ SystÃ¨me rÃ©silient
```

**Les 4 piliers:**
1. **DÃ©tection** - Savoir quand Ã§a casse
2. **Isolation** - EmpÃªcher la propagation
3. **RÃ©cupÃ©ration** - Revenir Ã  l'Ã©tat normal
4. **Apprentissage** - AmÃ©liorer continuellement

## Golden Signals

Les **Golden Signals** sont les 4 mÃ©triques essentielles pour monitorer n'importe quel service.

### Architecture de monitoring

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Service Ã  monitorer                   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Latency  â”‚  â”‚ Traffic  â”‚  â”‚  Errors  â”‚  â”‚Saturationâ”‚â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚ Prometheus â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚  Grafana   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Les 4 Signaux

**1. LATENCY (Latence)**
```
Request â†’ Processing â†’ Response
   â”‚          â”‚           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          Time = Latency

Mesures importantes:
- p50 (mÃ©diane) : 50% des requÃªtes plus rapides
- p95 : 95% des requÃªtes plus rapides
- p99 : 99% des requÃªtes plus rapides
- p99.9 : Pour dÃ©tecter les outliers
```

**Pourquoi les percentiles?**
```
Average = 100ms peut cacher:
â”œâ”€ 90% des requÃªtes @ 50ms  âœ“
â””â”€ 10% des requÃªtes @ 550ms âœ—

p95 = 200ms rÃ©vÃ¨le mieux la rÃ©alitÃ©
```

**2. TRAFFIC (Trafic)**
```
Volume de demandes par unitÃ© de temps

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RequÃªtes/sec au fil du      â”‚
â”‚             temps                â”‚
â”‚                                  â”‚
â”‚     â–²                           â”‚
â”‚ 500 â”‚      â•±â•²                   â”‚
â”‚     â”‚     â•±  â•²    â•±â•²            â”‚
â”‚ 250 â”‚â”€â”€â”€â”€â•±â”€â”€â”€â”€â•²â”€â”€â•±â”€â”€â•²â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚     â”‚   â•±      â•²â•±    â•²          â”‚
â”‚   0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º t  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      Normal   Spike  Normal
```

**3. ERRORS (Erreurs)**
```
Types d'erreurs Ã  tracker:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚HTTP Status  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2xx â†’ OK    â”‚
â”‚ 4xx â†’ Clientâ”‚ â† Valide mais Ã  monitorer
â”‚ 5xx â†’ Serverâ”‚ â† Erreur systÃ¨me critique
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Autres erreurs:
- Timeouts
- Panics/Crashes
- Exceptions non gÃ©rÃ©es
- Validation failures
```

**Calcul du taux d'erreur:**
```
                  RequÃªtes en erreur
Error Rate = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                 Total requÃªtes

Exemple: 15 erreurs / 10000 requÃªtes = 0.15% error rate
```

**4. SATURATION (Saturation)**
```
Utilisation des ressources critiques

CPU:     [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80%  âš ï¸
Memory:  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 60%  âœ“
Disk I/O:[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 90%  âš ï¸
Network: [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘] 30%  âœ“

Seuils typiques:
- 70%  â†’ Attention
- 85%  â†’ Alerte
- 95%  â†’ Critique
```

### Relation entre les signaux

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ TRAFFIC  â”‚
                    â”‚    â†‘     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LATENCY â”‚â”€â”€â”€â”€â–ºâ”‚ ERRORS  â”‚    â”‚SATURATIONâ”‚
    â”‚ augmenteâ”‚     â”‚augmententâ”‚    â”‚ augmente â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚               â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                          Cascading failure
```

### Dashboard type

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service: API Gateway                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Latency (p95)          â”‚ Traffic                   â”‚
â”‚ â–²                      â”‚ â–²                         â”‚
â”‚ â”‚  â•±â•²                  â”‚ â”‚    â•±â•²â•±â•²                â”‚
â”‚ â”‚ â•±  â•²                 â”‚ â”‚   â•±    â•²               â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â–º 245ms        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â–º 1.2k req/s    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Error Rate             â”‚ Saturation                â”‚
â”‚ â–²                      â”‚ CPU:   [â–ˆâ–ˆâ–ˆâ–‘] 35%         â”‚
â”‚ â”‚ â•±â•²                   â”‚ Mem:   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 52%        â”‚
â”‚ â”‚â•±  â•²                  â”‚ I/O:   [â–ˆâ–ˆâ–‘â–‘] 23%         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â–º 0.12%        â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## SLIs, SLOs, and SLAs

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HiÃ©rarchie                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  SLI (Indicator)                            â”‚
â”‚    â”‚                                        â”‚
â”‚    â”‚  "Comment on mesure?"                  â”‚
â”‚    â”‚  Ex: % requÃªtes < 200ms                â”‚
â”‚    â”‚                                        â”‚
â”‚    â–¼                                        â”‚
â”‚  SLO (Objective)                            â”‚
â”‚    â”‚                                        â”‚
â”‚    â”‚  "Quel est notre target?"              â”‚
â”‚    â”‚  Ex: 99.9% sur 30 jours                â”‚
â”‚    â”‚                                        â”‚
â”‚    â–¼                                        â”‚
â”‚  SLA (Agreement)                            â”‚
â”‚    â”‚                                        â”‚
â”‚    â”‚  "Qu'est-ce qu'on garantit?"           â”‚
â”‚    â”‚  Ex: 99.5% ou remboursement            â”‚
â”‚    â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SLI (Service Level Indicator)

MÃ©trique quantitative qui mesure un aspect du service.

```
Types de SLI:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Availability    â”‚ â†’ % de requÃªtes rÃ©ussies
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Latency         â”‚ â†’ % de requÃªtes < threshold
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Throughput      â”‚ â†’ RequÃªtes/sec traitÃ©es
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Correctness     â”‚ â†’ % de rÃ©sultats corrects
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemple de calcul:**
```
Availability SLI sur 1 heure:

Total requÃªtes:     10,000
RequÃªtes rÃ©ussies:   9,985
RequÃªtes Ã©chouÃ©es:      15

SLI = 9,985 / 10,000 = 0.9985 = 99.85%
```

### SLO (Service Level Objective)

Target interne pour un SLI. Structure: **SLI â‰¥ Target sur pÃ©riode**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Exemple de SLO                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                           â”‚
â”‚  99.9% des requÃªtes                       â”‚
â”‚  doivent rÃ©ussir                          â”‚
â”‚  sur une fenÃªtre glissante de 30 jours    â”‚
â”‚                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Composants:                              â”‚
â”‚  â€¢ MÃ©trique: % requÃªtes rÃ©ussies          â”‚
â”‚  â€¢ Target: 99.9%                          â”‚
â”‚  â€¢ PÃ©riode: 30 jours                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**FenÃªtre de mesure:**
```
Calendar-based (mensuel):
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Jan 1               Jan 31

Rolling window (30 jours glissants):
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    Aujourd'hui          -30j
```

### SLA (Service Level Agreement)

Contrat lÃ©gal avec consÃ©quences si non respectÃ©.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Relation SLO vs SLA             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  Performance actuelle:  99.99% âœ“       â”‚
â”‚         â”‚                              â”‚
â”‚         â”‚ (buffer)                     â”‚
â”‚         â–¼                              â”‚
â”‚  SLO interne:          99.9%  âœ“        â”‚
â”‚         â”‚                              â”‚
â”‚         â”‚ (marge de sÃ©curitÃ©)          â”‚
â”‚         â–¼                              â”‚
â”‚  SLA contractuel:      99.5%  âœ“        â”‚
â”‚         â”‚                              â”‚
â”‚         â–¼                              â”‚
â”‚  PÃ©nalitÃ©s             <99.5% âœ—        â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Principe: SLA < SLO < Performance
```

### Table de downtime autorisÃ©

| SLO    | Downtime/an | Downtime/mois | Downtime/semaine |
|--------|-------------|---------------|------------------|
| 90%    | 36.5j       | 3j            | 16.8h            |
| 99%    | 3.65j       | 7.2h          | 1.68h            |
| 99.9%  | 8.76h       | 43.8m         | 10.1m            |
| 99.95% | 4.38h       | 21.9m         | 5.04m            |
| 99.99% | 52.6m       | 4.38m         | 1.01m            |

**Calcul:**
```
SLO 99.9% sur 30 jours:

30 jours = 43,200 minutes
Uptime requis = 43,200 Ã— 0.999 = 43,156.8 min
Downtime autorisÃ© = 43,200 - 43,156.8 = 43.2 min
```

### Multiple SLOs

Services complexes ont plusieurs SLOs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        API Service SLOs                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚  Availability:  99.9%  (30d window)      â”‚
â”‚  Latency p95:   <200ms (99% du temps)    â”‚
â”‚  Latency p99:   <500ms (99.5% du temps)  â”‚
â”‚  Throughput:    >1000 req/s              â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Chaque SLO a son propre error budget
```

## Error Budgets

L'**error budget** est le temps de panne acceptable selon ton SLO.

### Concept

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Error Budget = 100% - SLO            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚  SLO 99.9% â†’ Error Budget 0.1%           â”‚
â”‚                                          â”‚
â”‚  Sur 30 jours:                           â”‚
â”‚  â€¢ 43,200 minutes total                  â”‚
â”‚  â€¢ 43.2 minutes de downtime OK           â”‚
â”‚  â€¢ 43,156.8 minutes uptime requis        â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fonctionnement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Mois en cours                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Jour 1-10:  Budget utilisÃ©: 10min / 43.2min   â”‚
â”‚              Reste: 33.2min  âœ“ Vert            â”‚
â”‚                                                â”‚
â”‚  Jour 11-15: Incident 20min                    â”‚
â”‚              Budget utilisÃ©: 30min / 43.2min   â”‚
â”‚              Reste: 13.2min  âš ï¸  Jaune         â”‚
â”‚                                                â”‚
â”‚  Jour 16-20: Incident 15min                    â”‚
â”‚              Budget utilisÃ©: 45min / 43.2min   â”‚
â”‚              Reste: -1.8min  âœ— Rouge           â”‚
â”‚              â†’ FREEZE FEATURES                 â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Policy de dÃ©cision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Ã‰tat Error Budget â†’ Action             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  > 50% restant  â†’ Ship rapide               â”‚
â”‚                   Prends des risques        â”‚
â”‚                   Innovation               â”‚
â”‚                                             â”‚
â”‚  20-50% restant â†’ Prudence                  â”‚
â”‚                   Review changements        â”‚
â”‚                   Tests supplÃ©mentaires    â”‚
â”‚                                             â”‚
â”‚  < 20% restant  â†’ Ralentir releases         â”‚
â”‚                   Focus sur fixes           â”‚
â”‚                   Postmortems              â”‚
â”‚                                             â”‚
â”‚  Ã‰puisÃ© (0%)    â†’ FREEZE TOTAL              â”‚
â”‚                   Bugs critiques only       â”‚
â”‚                   Root cause analysis      â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Burn Rate

Vitesse Ã  laquelle on consomme le budget.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Burn Rate Calculation             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚         Taux d'erreur actuel             â”‚
â”‚  BR = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚         Taux d'erreur autorisÃ©           â”‚
â”‚                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Exemples pour SLO 99.9%:                â”‚
â”‚                                          â”‚
â”‚  Taux actuel 0.1% â†’ BR = 0.1/0.1 = 1x    â”‚
â”‚  (Normal, budget Ã©puisÃ© en 30j)          â”‚
â”‚                                          â”‚
â”‚  Taux actuel 0.5% â†’ BR = 0.5/0.1 = 5x    â”‚
â”‚  (Rapide, budget Ã©puisÃ© en 6j)           â”‚
â”‚                                          â”‚
â”‚  Taux actuel 1.0% â†’ BR = 1.0/0.1 = 10x   â”‚
â”‚  (Critique, budget Ã©puisÃ© en 3j)         â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Window Alerting

StratÃ©gie pour dÃ©tecter les problÃ¨mes rapidement sans faux positifs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Multi-Window Multi-Burn-Rate               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  FenÃªtre    Burn Rate    Consomme budget  Alerte  â”‚
â”‚                                                    â”‚
â”‚  1 heure      14.4x        2% en 1h       2min    â”‚
â”‚  (rapide)                  Budget en 3j            â”‚
â”‚                                                    â”‚
â”‚  6 heures      6x          2% en 6h       15min   â”‚
â”‚  (moyen)                   Budget en 5j            â”‚
â”‚                                                    â”‚
â”‚  3 jours       1x         10% en 3j       1h      â”‚
â”‚  (lent)                    Budget en 30j           â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Principe: 
- FenÃªtre courte + burn rate Ã©levÃ© = Incident actif
- FenÃªtre longue + burn rate faible = DÃ©gradation lente
```

### Visualisation

```
Error Budget au fil du temps (30 jours)

 100% â”¤
      â”‚ â—â—â—
   75%â”‚    â—â—â—â—
      â”‚        â—â—
   50%â”‚          â—â—â—â—        DÃ©ploiement
      â”‚              â—â—      problÃ©matique
   25%â”‚                â—         â†“
      â”‚                 â—â—â—â—â—â—â—â—â—
    0%â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      1   5   10  15  20  25  30 (jours)
      
      âœ“ Jours 1-17: Budget sain
      âš ï¸  Jour 18: Incident commence
      âœ— Jours 19-30: Budget Ã©puisÃ©, freeze
```

### RÃ©cupÃ©ration du budget

```
Mode: Rolling Window (30 jours glissants)

Jour 1: Incident 20min
        Budget: 43.2 - 20 = 23.2min restant

Jour 31: La fenÃªtre glisse, Jour 1 sort
         Budget: Reset Ã  43.2min
         
C'est automatique avec rolling windows
```

## Chaos Engineering

Chaos engineering = injecter des pannes **volontairement** pour tester la rÃ©silience.

### Philosophie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "On casse pour apprendre avant que        â”‚
â”‚   la production ne casse toute seule"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HypothÃ¨se â†’ ExpÃ©rience â†’ Observation â†’ Apprentissage
```

### Les 5 Principes

```
1. DÃ©finir le Steady State
   â†“
   [SystÃ¨me normal: latence, errors, throughput]

2. HypothÃ¨se de rÃ©silience
   â†“
   "Le systÃ¨me devrait survivre si le pod X crash"

3. Injecter des pannes rÃ©elles
   â†“
   [Kill pod, add latency, partition network]

4. Observer la dÃ©viation
   â†“
   Est-ce que le steady state est maintenu?

5. Automatiser et rÃ©pÃ©ter
   â†“
   Game days, CI/CD chaos tests
```

### Blast Radius

ContrÃ´ler l'impact des expÃ©riences.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Progression du Chaos                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  1. Dev/Staging                                 â”‚
â”‚     â””â”€ 100% des ressources                      â”‚
â”‚        Risk: Bas                                â”‚
â”‚                                                 â”‚
â”‚  2. Production - Canary                         â”‚
â”‚     â””â”€ 1% du traffic                            â”‚
â”‚        Risk: ContrÃ´lÃ©                           â”‚
â”‚                                                 â”‚
â”‚  3. Production - RÃ©gion isolÃ©e                  â”‚
â”‚     â””â”€ 1 rÃ©gion sur 3                           â”‚
â”‚        Risk: ModÃ©rÃ©                             â”‚
â”‚                                                 â”‚
â”‚  4. Production - Full                           â”‚
â”‚     â””â”€ Toutes les rÃ©gions                       â”‚
â”‚        Risk: Ã‰levÃ© (game days only)             â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Types d'ExpÃ©riences

**1. Infrastructure**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pod/Container Level          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Pod termination              â”‚
â”‚ â€¢ Container kill               â”‚
â”‚ â€¢ Resource stress (CPU/Memory) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Network Level                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Latency injection            â”‚
â”‚ â€¢ Packet loss                  â”‚
â”‚ â€¢ Network partition            â”‚
â”‚ â€¢ DNS failures                 â”‚
â”‚ â€¢ Bandwidth limitation         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node Level                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Node drain                   â”‚
â”‚ â€¢ Node shutdown                â”‚
â”‚ â€¢ Disk pressure                â”‚
â”‚ â€¢ Clock skew                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. Application**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Service Degradation          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Slow responses               â”‚
â”‚ â€¢ Error injection (5xx)        â”‚
â”‚ â€¢ Timeout simulation           â”‚
â”‚ â€¢ Resource exhaustion          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Layer                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Database unavailability      â”‚
â”‚ â€¢ Slow queries                 â”‚
â”‚ â€¢ Connection pool exhaustion   â”‚
â”‚ â€¢ Corrupt data injection       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemple: Network Chaos

```
Ã‰tat Normal:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  10ms   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Service Aâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚Service Bâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     âœ“ OK

Injection Latency (100ms):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  110ms  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Service Aâ”‚â—„â•â•â•â•â•â•â•â–ºâ”‚Service Bâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     ? Test de timeout

Partition Network:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    âœ—    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Service Aâ”‚  â•±â•±â•±â•±â•±  â”‚Service Bâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     ? Test de fallback
```

### Anatomy d'une ExpÃ©rience Chaos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. BASELINE                                 â”‚
â”‚     Capturer mÃ©triques normales              â”‚
â”‚     (latency, errors, throughput)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. HYPOTHÃˆSE                                â”‚
â”‚     "Si on tue un pod, les autres prennent   â”‚
â”‚      le relais sans impact utilisateur"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. INJECTION                                â”‚
â”‚     kubectl delete pod service-x-abc123      â”‚
â”‚     + Observer pendant 5 minutes             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. VALIDATION                               â”‚
â”‚     âœ“ Latency p99 < 200ms maintenu          â”‚
â”‚     âœ“ Error rate < 0.1% maintenu            â”‚
â”‚     âœ“ Nouveau pod dÃ©marrÃ© en < 30s          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. CONCLUSION                               â”‚
â”‚     HypothÃ¨se validÃ©e âœ“                      â”‚
â”‚     OU                                       â”‚
â”‚     ProblÃ¨me dÃ©tectÃ© â†’ Action Items          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Game Days

Sessions d'entraÃ®nement pour incidents majeurs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Anatomy d'un Game Day             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Pre-Game (1 semaine avant)                 â”‚
â”‚  â”œâ”€ DÃ©finir scÃ©narios                       â”‚
â”‚  â”œâ”€ PrÃ©parer observabilitÃ©                  â”‚
â”‚  â”œâ”€ Brief Ã©quipe                            â”‚
â”‚  â””â”€ Review runbooks                         â”‚
â”‚                                             â”‚
â”‚  Game Day (2-3 heures)                      â”‚
â”‚  â”œâ”€ 10:00 - Baseline capture                â”‚
â”‚  â”œâ”€ 10:15 - Incident injection              â”‚
â”‚  â”‚          (ex: zone AWS down)             â”‚
â”‚  â”œâ”€ 10:16 - Ã‰quipe dÃ©tecte                  â”‚
â”‚  â”œâ”€ 10:20 - Debug & mitigation              â”‚
â”‚  â”œâ”€ 10:45 - RÃ©solution                      â”‚
â”‚  â””â”€ 11:00 - Service restaurÃ©                â”‚
â”‚                                             â”‚
â”‚  Post-Game (immÃ©diat)                       â”‚
â”‚  â”œâ”€ Debrief Ã  chaud (30min)                 â”‚
â”‚  â”œâ”€ Ce qui a marchÃ©                         â”‚
â”‚  â”œâ”€ Ce qui a Ã©chouÃ©                         â”‚
â”‚  â””â”€ Action items                            â”‚
â”‚                                             â”‚
â”‚  Post-Mortem (3 jours aprÃ¨s)                â”‚
â”‚  â”œâ”€ Document complet                        â”‚
â”‚  â”œâ”€ Timeline dÃ©taillÃ©e                      â”‚
â”‚  â”œâ”€ Root cause                              â”‚
â”‚  â””â”€ Preventive measures                     â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Outils Chaos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Chaos Mesh (CNCF)               â”‚
â”‚  â”œâ”€ Pod Chaos                           â”‚
â”‚  â”œâ”€ Network Chaos                       â”‚
â”‚  â”œâ”€ IO Chaos                            â”‚
â”‚  â”œâ”€ Stress Chaos                        â”‚
â”‚  â”œâ”€ Time Chaos                          â”‚
â”‚  â””â”€ Kernel Chaos                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Litmus (CNCF)                   â”‚
â”‚  â”œâ”€ Pre-built experiments               â”‚
â”‚  â”œâ”€ Chaos workflows                     â”‚
â”‚  â””â”€ Chaos metrics                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Toxiproxy                       â”‚
â”‚  â”œâ”€ Network proxy                       â”‚
â”‚  â”œâ”€ Latency injection                   â”‚
â”‚  â”œâ”€ Bandwidth limitation                â”‚
â”‚  â””â”€ Connection failures                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸš€ Practical Examples

Ready-to-use Chaos Mesh examples are available in the [`chaos-mesh/`](./chaos-mesh/) directory:

- **Installation script** - Quick setup of Chaos Mesh on your cluster
- **Pod Kill** - Test pod restart resilience
- **Network Delay** - Simulate network latency
- **Pod Failure** - Make pods unavailable
- **Stress Test** - CPU and memory pressure testing

See [`chaos-mesh/README.md`](./chaos-mesh/README.md) for detailed instructions and usage examples.

### Scheduling Chaos

```
FrÃ©quence recommandÃ©e:

Dev/Staging:
â””â”€ Continuous (chaque deploy)

Production - Automated:
â”œâ”€ Pod kills: Quotidien
â”œâ”€ Network chaos: Hebdomadaire
â””â”€ Node drain: Hebdomadaire

Production - Game Days:
â””â”€ Major scenarios: Mensuel ou trimestriel
```

## Circuit Breakers and Rate Limiting

Patterns pour prÃ©venir les cascades de pannes et protÃ©ger les ressources.

### Circuit Breaker

Coupe temporairement les appels Ã  un service dÃ©faillant pour Ã©viter la surcharge.

**Ã‰tats du Circuit Breaker:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Circuit States                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Ã‰tat CLOSED (Normal)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Requests OK   â”‚
    â”‚   âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Trop d'erreurs (ex: 50% sur 10 req)
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Ã‰tat OPEN     â”‚
    â”‚   Fail Fast     â”‚
    â”‚   âœ—âœ—âœ—âœ—âœ—âœ—âœ—âœ—     â”‚ â† Rejette immÃ©diatement
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ AprÃ¨s timeout (ex: 60s)
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Ã‰tat HALF-OPEN  â”‚
    â”‚ Teste avec      â”‚
    â”‚ N requÃªtes      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
        â”‚         â”‚
    SuccÃ¨s    Ã‰chec
        â”‚         â”‚
        â–¼         â–¼
    CLOSED     OPEN
```

**Timeline d'un incident:**

```
Time â†’

00:00  [CLOSED] âœ“âœ“âœ“âœ“âœ“âœ“  Tout OK
       â”‚
00:05  [CLOSED] âœ“âœ“âœ—âœ—âœ—âœ—  Service commence Ã  fail
       â”‚
00:06  [OPEN]   Circuit s'ouvre
       â”‚        RequÃªtes rejetÃ©es immÃ©diatement
       â”‚        Pas de charge sur service dÃ©faillant
       â”‚
01:06  [HALF-OPEN] Test avec 3 requÃªtes
       â”‚
       â”œâ”€ Si âœ“âœ“âœ“ â†’ [CLOSED] Reprise normale
       â”‚
       â””â”€ Si âœ—âœ—âœ— â†’ [OPEN] Reste fermÃ© 60s de plus
```

**Configuration typique:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Circuit Breaker Settings          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚
â”‚  Failure Threshold: 50%            â”‚
â”‚  (% Ã©checs pour ouvrir)            â”‚
â”‚                                    â”‚
â”‚  Request Threshold: 10             â”‚
â”‚  (minimum de requÃªtes Ã  analyser)  â”‚
â”‚                                    â”‚
â”‚  Timeout: 60s                      â”‚
â”‚  (durÃ©e avant HALF-OPEN)           â”‚
â”‚                                    â”‚
â”‚  Half-Open Requests: 3             â”‚
â”‚  (requÃªtes test)                   â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cascade Failure Prevention

Pourquoi le circuit breaker est critique:

```
Sans Circuit Breaker:
â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ API  â”‚â”€â”€â”€â–ºâ”‚ DB   â”‚    â”‚Users â”‚
â”‚      â”‚â—„â”€â”€â”€â”‚(slow)â”‚    â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜
   â”‚                        â”‚
   â”‚ Accumule requÃªtes      â”‚
   â”‚ Threads bloquÃ©s        â”‚
   â”‚ Memory overflow        â”‚
   â–¼                        â–¼
  CRASH  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  All fails

Avec Circuit Breaker:
â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ API  â”‚ âœ—  â”‚ DB   â”‚    â”‚Users â”‚
â”‚      â”‚    â”‚(slow)â”‚    â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜
   â”‚                        â”‚
   â”‚ Circuit OPEN           â”‚
   â”‚ Fail fast              â”‚
   â”‚ Ressources libÃ©rÃ©es    â”‚
   â–¼                        â–¼
  OK     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Partial service
                        (cache, fallback)
```

### Rate Limiting

Limite le nombre de requÃªtes acceptÃ©es sur une pÃ©riode.

**Algorithmes:**

**1. Token Bucket**
```
Bucket capacity: 100 tokens
Refill rate: 10 tokens/sec

    Bucket [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80 tokens
              â†‘
              â”‚ +10/sec
              â”‚
    Request consomme 1 token
              â†“
    Accept si token disponible
    Reject si bucket vide

Avantage: Permet des bursts
```

**2. Leaky Bucket**
```
    Requests arrivent
         â†“â†“â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ Buffer
    â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
    â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚ Constant rate
         â†“
    Processed

Avantage: Lisse le trafic
```

**3. Fixed Window**
```
Window: 1 minute
Limit: 100 requests

10:00:00 - 10:00:59  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 75 req  âœ“
10:01:00 - 10:01:59  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100 req âœ“
10:02:00 - 10:02:59  [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 40 req  âœ“

ProblÃ¨me: Burst Ã  la frontiÃ¨re
10:00:30 - 50 req âœ“
10:01:00 - 100 req âœ“
â†’ 150 req en 30 secondes!
```

**4. Sliding Window**
```
Current time: 10:05:30
Window: 1 minute glissant

10:04:30 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 10:05:30
     [Compte requÃªtes dans cette fenÃªtre]

Plus prÃ©cis, pas de burst aux frontiÃ¨res
```

### Rate Limiting Strategies

**Per-User Limiting:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User A: 100 req/min           â”‚
â”‚  User B: 100 req/min           â”‚
â”‚  User C: 1000 req/min (premium)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EmpÃªche qu'un user monopolise le service
```

**Global Limiting:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Total API: 10,000 req/sec     â”‚
â”‚                                â”‚
â”‚  ProtÃ¨ge l'infra globale       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tiered Limiting:**
```
Request Path:

1. WAF/CDN: 100k req/s
           â†“
2. Load Balancer: 50k req/s
           â†“
3. API Gateway: 10k req/s
           â†“
4. Service: 1k req/s
           â†“
5. Database: 500 queries/s

Chaque layer protÃ¨ge le suivant
```

### Response Headers

```
HTTP/1.1 429 Too Many Requests
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1640000000
Retry-After: 60

Body:
{
  "error": "rate_limit_exceeded",
  "message": "Try again in 60 seconds"
}
```

### Kubernetes Rate Limiting

**Envoy/Istio Level:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Envoy Sidecar             â”‚
â”‚                              â”‚
â”‚  Local rate limit:           â”‚
â”‚  - 1000 tokens/sec           â”‚
â”‚  - Burst: 2000               â”‚
â”‚                              â”‚
â”‚  Global rate limit:          â”‚
â”‚  - Redis backend             â”‚
â”‚  - Distributed counter       â”‚
â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ingress Level:**
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/limit-burst-multiplier: "5"
```

## Retry Strategies

RÃ©essayer intelligemment les opÃ©rations qui Ã©chouent.

### Exponential Backoff

DÃ©lai qui augmente exponentiellement entre chaque retry.

```
Attempt 1: Immediate
           â†“ Fail
           Wait 1s
           
Attempt 2: After 1s
           â†“ Fail
           Wait 2s
           
Attempt 3: After 2s
           â†“ Fail
           Wait 4s
           
Attempt 4: After 4s
           â†“ Fail
           Wait 8s
           
Attempt 5: After 8s
           â†“ Fail
           Give up

Formule: delay = base Ã— 2^(attempt - 1)
```

**Avec Jitter:**

```
Sans jitter (problÃ¨me):
Time â†’
10:00:00 â†’ 1000 clients retry en mÃªme temps
           â”‚â”‚â”‚â”‚â”‚â”‚â”‚â”‚â”‚
           â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
           Thundering herd!
           Server overwhelmed

Avec jitter (solution):
10:00:00 â†’ Clients retry avec dÃ©lais variÃ©s
           â”‚ â”‚  â”‚ â”‚  â”‚ â”‚  â”‚ â”‚
           â–¼ â–¼  â–¼ â–¼  â–¼ â–¼  â–¼ â–¼
           Spread load
           Server OK

Jitter = delay Â± random(0, delay/2)
```

**Visualisation:**

```
Delay (secondes)

64 â”‚                              â—
   â”‚                            /
32 â”‚                        â—
   â”‚                      /
16 â”‚                  â—
   â”‚                /
 8 â”‚            â—
   â”‚          /
 4 â”‚      â—
   â”‚    /
 2 â”‚  â—
   â”‚/
 1 â—
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
   1  2  3  4  5  6  7  8  9  10 (attempts)
   
   Avec cap maximum Ã  60s
```

### Decision Tree pour Retries

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpÃ©ration Ã©choue      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Erreur type?  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚
    â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Transientâ”‚    â”‚Permanent â”‚
â”‚(temp)   â”‚    â”‚(persist) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚
     â”‚              â–¼
     â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚         â”‚ STOP    â”‚
     â”‚         â”‚No Retry â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Idempotent?      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
  â”Œâ”€â”€â”´â”€â”€â”€â”
  â”‚      â”‚
  â–¼      â–¼
 Oui    Non
  â”‚      â”‚
  â”‚      â””â”€â”€â†’ STOP (risque de duplicate)
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retry avec       â”‚
â”‚ exponential      â”‚
â”‚ backoff + jitter â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Erreurs: Transient vs Permanent

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Erreurs Transient (Retry)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Network timeout                   â”‚
â”‚ â€¢ Connection refused                â”‚
â”‚ â€¢ 503 Service Unavailable           â”‚
â”‚ â€¢ 429 Too Many Requests             â”‚
â”‚ â€¢ 500 Internal Server Error         â”‚
â”‚ â€¢ Deadlock (DB)                     â”‚
â”‚ â€¢ Temporary unavailability          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Erreurs Permanent (No Retry)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 400 Bad Request                   â”‚
â”‚ â€¢ 401 Unauthorized                  â”‚
â”‚ â€¢ 403 Forbidden                     â”‚
â”‚ â€¢ 404 Not Found                     â”‚
â”‚ â€¢ 422 Unprocessable Entity          â”‚
â”‚ â€¢ Validation errors                 â”‚
â”‚ â€¢ Data corruption                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Idempotence

**Critique:** Les opÃ©rations retryÃ©es DOIVENT Ãªtre idempotentes.

```
Non-Idempotent (DANGER):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IncrementCounter()       â”‚
â”‚   counter = counter + 1  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Appel 1: counter = 5 â†’ 6  âœ“
Retry:   counter = 6 â†’ 7  âœ— (erreur!)

Idempotent (SAFE):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SetCounter(value)        â”‚
â”‚   counter = value        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Appel 1: counter = 6      âœ“
Retry:   counter = 6      âœ“ (mÃªme rÃ©sultat)
```

**Idempotency Key Pattern:**

```
Request avec clÃ© unique:
POST /payments
{
  "idempotency_key": "pay_abc123",
  "amount": 100
}

Serveur:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Check si key existe         â”‚
â”‚    â””â”€ Oui â†’ Return cached resultâ”‚
â”‚    â””â”€ Non â†’ Process payment    â”‚
â”‚                                â”‚
â”‚ 2. Process payment             â”‚
â”‚                                â”‚
â”‚ 3. Store result avec key       â”‚
â”‚                                â”‚
â”‚ 4. Return result               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Retry avec mÃªme key:
â†’ Returns cached result
â†’ No double payment âœ“
```

### Timeouts

**Toujours** dÃ©finir des timeouts stricts.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Timeout Hierarchy               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚  Request Timeout: 30s                â”‚
â”‚    â”‚                                 â”‚
â”‚    â”œâ”€ Connection Timeout: 5s         â”‚
â”‚    â”œâ”€ TLS Handshake: 5s              â”‚
â”‚    â”œâ”€ Response Header: 10s           â”‚
â”‚    â””â”€ Read Timeout: 20s              â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Principe: Chaque phase a son timeout
```

**Timeout Cascade:**

```
Client â†’ LB â†’ API â†’ DB
 30s     25s   20s   10s

Chaque hop rÃ©duit le timeout
pour laisser du temps aux layers prÃ©cÃ©dents
```

### Retry Budget

Limiter les retries pour Ã©viter l'amplification.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Retry Budget                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚  RequÃªtes originales:  1000/sec      â”‚
â”‚  Retry budget:         20% = 200/sec â”‚
â”‚                                      â”‚
â”‚  Si toutes les requÃªtes fail:        â”‚
â”‚  â†’ Max 200 retries/sec               â”‚
â”‚  â†’ Pas 1000 retries                  â”‚
â”‚                                      â”‚
â”‚  Ã‰vite d'aggraver la situation       â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Best Practices

```
âœ“ DO:
â”œâ”€ Use exponential backoff
â”œâ”€ Add jitter (Â±50%)
â”œâ”€ Set max retry count (3-5)
â”œâ”€ Set max delay cap (60s)
â”œâ”€ Use context.Context pour cancellation
â”œâ”€ Log retry attempts
â”œâ”€ Monitor retry rate
â”œâ”€ Only retry idempotent operations
â””â”€ Use idempotency keys

âœ— DON'T:
â”œâ”€ Retry indefinitely
â”œâ”€ Retry without backoff (hammering)
â”œâ”€ Retry non-idempotent ops
â”œâ”€ Retry permanent errors
â”œâ”€ Ignore retry budget
â””â”€ Retry without timeout
```

## Graceful Degradation

DÃ©grader les fonctionnalitÃ©s progressivement plutÃ´t que de tout casser.

### Principe

```
SystÃ¨me Brittle (fragile):
Feature A â†’ Dependency fails â†’ Everything fails âœ—

SystÃ¨me Resilient:
Feature A â†’ Dependency fails â†’ Fallback to basic version âœ“
```

### Degradation Ladder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Performance Degradation           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  100% â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Full features    â”‚
â”‚       â”‚ â€¢ ML recommendations           â”‚
â”‚       â”‚ â€¢ Real-time data               â”‚
â”‚       â”‚ â€¢ Personalization              â”‚
â”‚       â”‚                                â”‚
â”‚   75% â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ Reduced features  â”‚
â”‚       â”‚ â€¢ Static recommendations       â”‚
â”‚       â”‚ â€¢ Cached data (5min)           â”‚
â”‚       â”‚ â€¢ Basic personalization        â”‚
â”‚       â”‚                                â”‚
â”‚   50% â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ Core features     â”‚
â”‚       â”‚ â€¢ Popular items only           â”‚
â”‚       â”‚ â€¢ Cached data (1h)             â”‚
â”‚       â”‚ â€¢ No personalization           â”‚
â”‚       â”‚                                â”‚
â”‚   25% â”‚ â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Minimal           â”‚
â”‚       â”‚ â€¢ Static content               â”‚
â”‚       â”‚ â€¢ Stale cache OK               â”‚
â”‚       â”‚                                â”‚
â”‚    0% â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Failure           â”‚
â”‚       â”‚ â€¢ Service down                 â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### StratÃ©gies

**1. Feature Flags**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Feature Flag Decision Tree        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GetRecommendations(userID)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ML Service UP?   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚         â”‚
    YES       NO
     â”‚         â”‚
     â–¼         â–¼
  ML Reco   Popular
  (best)    (fallback)
     â”‚         â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â–¼
      Return data
```

**2. Tiered Fallbacks**

```
Level 1: Real-time ML
         â†“ (fails)
Level 2: Cached ML (1h old)
         â†“ (fails)
Level 3: Rule-based recommendations
         â†“ (fails)
Level 4: Popular items
         â†“ (fails)
Level 5: Static defaults

Chaque niveau est "moins bon" mais disponible
```

**3. Cache with Stale Data**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Cache Freshness Strategy      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚
â”‚  Fresh (< 5min):   Serve directly  â”‚
â”‚  Stale (< 1h):     Serve + Refresh â”‚
â”‚  Very stale (< 1d): Serve if DB downâ”‚
â”‚  Expired (> 1d):   Reject          â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline:
0â”€â”€â”€â”€5minâ”€â”€â”€â”€1hâ”€â”€â”€â”€â”€â”€â”€â”€1dâ”€â”€â”€â”€â–º time
â”‚    â”‚       â”‚          â”‚
Freshâ”‚  Stale â”‚  Very    â”‚ Expired
     â”‚  (OK)  â”‚  stale   â”‚
     â”‚        â”‚  (if     â”‚
     â”‚        â”‚  needed) â”‚
```

**Flow avec stale data:**

```
Request â†’ Check cache
          â”‚
          â”œâ”€ Fresh? â†’ Return âœ“
          â”‚
          â”œâ”€ Stale? â†’ Return stale âš ï¸
          â”‚           + Async refresh
          â”‚
          â””â”€ Expired? â†’ Try DB
                        â”‚
                        â”œâ”€ DB OK â†’ Return fresh âœ“
                        â”‚
                        â””â”€ DB down â†’ Return stale anyway âš ï¸
                                     (better than nothing)
```

**4. Partial Responses**

```
Dashboard Request:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Component 1: User Info         â”‚ âœ“ Success (50ms)
â”‚  Component 2: Stats             â”‚ âœ“ Success (120ms)
â”‚  Component 3: Recommendations   â”‚ âœ— Failed (timeout)
â”‚  Component 4: Notifications     â”‚ âœ“ Success (80ms)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Response:
{
  "user_info": { ... },        // âœ“
  "stats": { ... },            // âœ“
  "recommendations": null,     // âœ— (mais pas de blocage)
  "notifications": [ ... ],    // âœ“
  "errors": [
    "recommendations temporarily unavailable"
  ]
}

3/4 composants fonctionnent = Partial success
```

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Parallel Fetching              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Request
   â”‚
   â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
   â”‚     â”‚     â”‚     â”‚     â”‚
   â–¼     â–¼     â–¼     â–¼     â–¼
  API1  API2  API3  API4  API5
 (5s)  (10s) (20s) (3s)  (15s)
   â”‚     â”‚     â”‚     â”‚     â”‚
   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
            â”‚
         Timeout: 12s
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚
 Success          Timeout
 (API1,4,2)       (API3,5)
    â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
    Partial response
    + Error details
```

### Load Shedding

Rejeter volontairement des requÃªtes pour protÃ©ger le systÃ¨me.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Load Shedding Thresholds        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  Load < 70%:  Accept all              â”‚
â”‚               â”‚                        â”‚
â”‚  Load 70-85%: Start shedding          â”‚
â”‚               â”‚ â€¢ Drop low-priority    â”‚
â”‚               â”‚ â€¢ Serve from cache     â”‚
â”‚               â”‚                        â”‚
â”‚  Load 85-95%: Aggressive shedding     â”‚
â”‚               â”‚ â€¢ Only critical users  â”‚
â”‚               â”‚ â€¢ Basic features only  â”‚
â”‚               â”‚                        â”‚
â”‚  Load > 95%:  Emergency mode          â”‚
â”‚               â””â”€ Maintenance page      â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Priority-based shedding:**

```
Request Classification:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  P0: Critical           â”‚ â†’ Always serve
â”‚      (payments, auth)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P1: Important          â”‚ â†’ Serve if < 85% load
â”‚      (core features)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P2: Nice-to-have       â”‚ â†’ Serve if < 70% load
â”‚      (analytics)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P3: Optional           â”‚ â†’ Serve if < 50% load
â”‚      (recommendations)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Bulkhead Pattern

Isoler les ressources pour Ã©viter qu'une panne ne contamine tout.

```
Without Bulkheads (bad):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Shared Thread Pool     â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â”‚
â”‚   All 100 threads used   â”‚
â”‚   by slow DB queries     â”‚
â”‚   â†’ No threads for API   â”‚
â”‚   â†’ Everything blocked   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With Bulkheads (good):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DB Pool      â”‚ API Pool â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚ â–ˆâ–ˆâ–ˆâ–ˆ     â”‚
â”‚  50 threads   â”‚ 30 thds  â”‚
â”‚  (saturated)  â”‚ (OK)     â”‚
â”‚               â”‚          â”‚
â”‚  âœ— DB slow    â”‚ âœ“ API OK â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Kubernetes example:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Pod Resource Isolation        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚
â”‚  Critical Service:                 â”‚
â”‚  â”œâ”€ requests: 2 CPU, 4Gi          â”‚
â”‚  â”œâ”€ limits: 4 CPU, 8Gi            â”‚
â”‚  â””â”€ guaranteed QoS                 â”‚
â”‚                                    â”‚
â”‚  Background Jobs:                  â”‚
â”‚  â”œâ”€ requests: 0.5 CPU, 1Gi        â”‚
â”‚  â”œâ”€ limits: 1 CPU, 2Gi            â”‚
â”‚  â””â”€ burstable QoS                  â”‚
â”‚                                    â”‚
â”‚  â†’ Jobs can't starve critical      â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Observability

Les **3 piliers** de l'observabilitÃ©.

### Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Observability Triangle             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              METRICS
                 â–²
                â•± â•²
               â•±   â•²
              â•±     â•²
             â•±       â•²
            â•± System  â•²
           â•±  Health   â•²
          â•±             â•²
         â•±               â•²
        â•±                 â•²
    LOGS â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º TRACES

Chaque pilier rÃ©pond Ã  une question:
- Metrics: "Qu'est-ce qui ne va pas?"
- Logs: "Pourquoi Ã§a ne va pas?"
- Traces: "OÃ¹ est le problÃ¨me?"
```

### 1. Metrics

DonnÃ©es numÃ©riques agrÃ©gÃ©es dans le temps.

**Types de mÃ©triques:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Counter                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Toujours en augmentation       â”‚
â”‚  Ex: requests_total             â”‚
â”‚                                 â”‚
â”‚      â–²                          â”‚
â”‚  500 â”‚        â•±                 â”‚
â”‚  250 â”‚      â•±                   â”‚
â”‚    0 â””â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º time     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gauge                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Peut monter ou descendre       â”‚
â”‚  Ex: cpu_usage, memory          â”‚
â”‚                                 â”‚
â”‚      â–²    â•±â•²                    â”‚
â”‚  100 â”‚   â•±  â•²  â•±â•²               â”‚
â”‚   50 â”‚  â•±    â•²â•±  â•²              â”‚
â”‚    0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º time   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Histogram                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Distribution de valeurs        â”‚
â”‚  Ex: request_duration           â”‚
â”‚                                 â”‚
â”‚      â–²                          â”‚
â”‚  freqâ”‚    â–ˆâ–ˆâ–ˆ                   â”‚
â”‚      â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   â”‚
â”‚      â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º latency    â”‚
â”‚        p50 p95 p99              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Summary                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Percentiles prÃ©calculÃ©s        â”‚
â”‚  Ex: quantiles Ï†(0.5, 0.9, 0.99)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Instrumentation flow:**

```
Application
    â”‚
    â”œâ”€ httpDuration.Observe(0.234)
    â”œâ”€ requestsTotal.Inc()
    â”œâ”€ activeConnections.Set(42)
    â”‚
    â–¼
Prometheus Client Library
    â”‚
    â–¼
/metrics endpoint
    â”‚
    â–¼
Prometheus Server (scrape every 15s)
    â”‚
    â–¼
Storage (TSDB)
    â”‚
    â–¼
Grafana (visualization)
```

**Cardinality:**

```
ATTENTION: Cardinality explosion!

âŒ BAD:
http_requests{user_id="12345", ...}
â†’ Millions de users = Millions de series
â†’ Prometheus OOM

âœ“ GOOD:
http_requests{endpoint="/api/users", status="200"}
â†’ Cardinality limitÃ©e
â†’ Use logging pour user_id
```

### 2. Logs

Ã‰vÃ©nements discrets avec contexte.

**Log Levels:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Severity Pyramid                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         FATAL   â† Service crash
           â”‚
         ERROR   â† Needs immediate attention
           â”‚
          WARN   â† Unexpected but handled
           â”‚
          INFO   â† Normal operations
           â”‚
         DEBUG   â† Development details
           â”‚
         TRACE   â† Ultra-detailed
```

**Structured Logging:**

```
Unstructured (bad):
"User john logged in from 192.168.1.1 at 10:05"
â†’ Hard to parse, search, aggregate

Structured (good):
{
  "timestamp": "2024-11-30T10:05:00Z",
  "level": "INFO",
  "msg": "user login",
  "user_id": "john",
  "ip": "192.168.1.1",
  "trace_id": "abc123"
}
â†’ Easy to query, correlate, analyze
```

**Log Pipeline:**

```
Application
    â”‚ (structured logs)
    â–¼
Stdout/Stderr
    â”‚
    â–¼
Log Collector (Fluent Bit, Fluentd)
    â”‚
    â”œâ”€ Parse
    â”œâ”€ Enrich (add k8s metadata)
    â”œâ”€ Filter
    â””â”€ Buffer
    â”‚
    â–¼
Log Storage (Loki, ElasticSearch)
    â”‚
    â–¼
Query Interface (Grafana, Kibana)
```

**What to log:**

```
âœ“ DO Log:
â”œâ”€ Service start/stop
â”œâ”€ Significant state changes
â”œâ”€ External API calls (with duration)
â”œâ”€ Authentication events
â”œâ”€ Errors with stack traces
â”œâ”€ Business events
â””â”€ Request IDs for tracing

âœ— DON'T Log:
â”œâ”€ Sensitive data (passwords, tokens)
â”œâ”€ High-frequency events (every cache hit)
â”œâ”€ Redundant info (already in metrics)
â””â”€ Binary data
```

### 3. Traces

Suivi d'une requÃªte Ã  travers tout le systÃ¨me distribuÃ©.

**Distributed Trace:**

```
User Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway         [Span A: 245ms] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚          â”‚
          â–¼          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Auth    â”‚  â”‚ UserSvc  â”‚
    â”‚[Span B] â”‚  â”‚[Span C]  â”‚
    â”‚  25ms   â”‚  â”‚  180ms   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ DB      â”‚
                 â”‚[Span D] â”‚
                 â”‚  120ms  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Trace = Collection of Spans
Span = Single operation with start/end
```

**Trace Anatomy:**

```
Trace ID: abc-123-def
â”‚
â”œâ”€ Span ID: span-1 (API Gateway)
â”‚  â”œâ”€ Start: 10:05:00.000
â”‚  â”œâ”€ End: 10:05:00.245
â”‚  â”œâ”€ Duration: 245ms
â”‚  â”œâ”€ Tags: {service: api-gateway, http.method: GET}
â”‚  â””â”€ Children: [span-2, span-3]
â”‚
â”œâ”€ Span ID: span-2 (Auth Service)
â”‚  â”œâ”€ Parent: span-1
â”‚  â”œâ”€ Duration: 25ms
â”‚  â””â”€ Tags: {service: auth}
â”‚
â””â”€ Span ID: span-3 (User Service)
   â”œâ”€ Parent: span-1
   â”œâ”€ Duration: 180ms
   â”œâ”€ Children: [span-4]
   â””â”€ Tags: {service: user}
       â”‚
       â””â”€ Span ID: span-4 (Database)
          â”œâ”€ Parent: span-3
          â”œâ”€ Duration: 120ms
          â””â”€ Tags: {service: postgres, query: SELECT}
```

**Waterfall View:**

```
Time â†’
0ms        100ms       200ms       300ms

API GW  [â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•] 245ms
  â”‚
  â”œâ”€Auth [â•â•â•] 25ms
  â”‚
  â””â”€User     [â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•] 180ms
       â”‚
       â””â”€DB        [â•â•â•â•â•â•â•â•] 120ms

Insights:
- Total latency: 245ms
- DB is slowest (120ms)
- User service has 60ms overhead
```

### Correlation

Relier metrics, logs, traces.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Incident Timeline                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚ 10:05 â”‚ Metrics: p99 latency spike   â”‚
â”‚       â”‚          500ms â†’ 2000ms      â”‚
â”‚       â”‚                              â”‚
â”‚       â–¼                              â”‚
â”‚       Search logs with:              â”‚
â”‚       - timestamp ~10:05             â”‚
â”‚       - latency > 1s                 â”‚
â”‚       â”‚                              â”‚
â”‚       â”œâ”€ Found trace_id: abc123      â”‚
â”‚       â”‚                              â”‚
â”‚       â–¼                              â”‚
â”‚       View trace abc123:             â”‚
â”‚       - DB query: 1.8s (slow!)       â”‚
â”‚       - Query: SELECT * FROM ...     â”‚
â”‚       â”‚                              â”‚
â”‚       â–¼                              â”‚
â”‚       Root cause: Missing index      â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flow: Metrics â†’ Logs â†’ Traces â†’ Root Cause
```

**Unified Observability:**

```
Grafana Dashboard
â”œâ”€ Panel 1: Metrics (Golden Signals)
â”‚  â””â”€ Click anomaly â†’ Filter logs
â”‚
â”œâ”€ Panel 2: Logs (filtered by time)
â”‚  â””â”€ Click log â†’ Show trace
â”‚
â””â”€ Panel 3: Trace (distributed view)
   â””â”€ Identify slow span

Single pane of glass for troubleshooting
```

### Sampling

Traces peuvent Ãªtre coÃ»teux - Ã©chantillonner intelligemment.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Sampling Strategies           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚
â”‚ 1. Head-based (dÃ©cision au dÃ©but)  â”‚
â”‚    Sample 1% of all requests       â”‚
â”‚    â”œâ”€ Pro: Simple, low overhead    â”‚
â”‚    â””â”€ Con: Peut rater les erreurs  â”‚
â”‚                                    â”‚
â”‚ 2. Tail-based (dÃ©cision Ã  la fin)  â”‚
â”‚    Keep all errors + 1% success    â”‚
â”‚    â”œâ”€ Pro: Catch all issues        â”‚
â”‚    â””â”€ Con: Complex, more overhead  â”‚
â”‚                                    â”‚
â”‚ 3. Adaptive                        â”‚
â”‚    Increase sampling when errorsâ†‘  â”‚
â”‚    â”œâ”€ Pro: Best of both            â”‚
â”‚    â””â”€ Con: Most complex            â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Deployment Strategies

StratÃ©gies pour dÃ©ployer des changements sans risque.

### 1. Rolling Update

Le dÃ©faut dans Kubernetes - remplacement progressif.

```
Initial state:
[v1] [v1] [v1] [v1] [v1]

Step 1: Kill 1 old, start 1 new
[v1] [v1] [v1] [v1] [v2]

Step 2:
[v1] [v1] [v1] [v2] [v2]

Step 3:
[v1] [v1] [v2] [v2] [v2]

Step 4:
[v1] [v2] [v2] [v2] [v2]

Step 5:
[v2] [v2] [v2] [v2] [v2]

Config:
maxUnavailable: 1 (max pods down)
maxSurge: 1 (max extra pods)
```

**Timeline:**

```
Time â†’
0s     30s    60s    90s    120s   150s

v1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
v2: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

Traffic gradually shifts from v1 to v2
Both versions coexist during rollout
```

### 2. Blue-Green Deployment

Deux environnements identiques - switch instantanÃ©.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Before Deployment          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                 â”‚
â”‚  Load Balancer                  â”‚
â”‚        â”‚                        â”‚
â”‚        â–¼                        â”‚
â”‚   Blue (v1) âœ“                   â”‚
â”‚   [pod] [pod] [pod]             â”‚
â”‚   100% traffic                  â”‚
â”‚                                 â”‚
â”‚   Green (v2)                    â”‚
â”‚   [pod] [pod] [pod]             â”‚
â”‚   0% traffic (testing only)     â”‚
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      After Switch               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                 â”‚
â”‚  Load Balancer                  â”‚
â”‚        â”‚                        â”‚
â”‚        â–¼                        â”‚
â”‚   Blue (v1)                     â”‚
â”‚   [pod] [pod] [pod]             â”‚
â”‚   0% traffic (kept for rollback)â”‚
â”‚                                 â”‚
â”‚   Green (v2) âœ“                  â”‚
â”‚   [pod] [pod] [pod]             â”‚
â”‚   100% traffic                  â”‚
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Rollback: Just switch back to Blue
```

### 3. Canary Deployment

DÃ©ploiement progressif avec monitoring.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Canary Progression             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚
â”‚ Phase 1: 5% traffic to canary      â”‚
â”‚ â”œâ”€ Monitor for 10 minutes          â”‚
â”‚ â”œâ”€ Check error rate, latency       â”‚
â”‚ â””â”€ If OK â†’ continue                â”‚
â”‚                                    â”‚
â”‚ Phase 2: 25% traffic               â”‚
â”‚ â”œâ”€ Monitor for 10 minutes          â”‚
â”‚ â””â”€ If OK â†’ continue                â”‚
â”‚                                    â”‚
â”‚ Phase 3: 50% traffic               â”‚
â”‚ â”œâ”€ Monitor for 10 minutes          â”‚
â”‚ â””â”€ If OK â†’ continue                â”‚
â”‚                                    â”‚
â”‚ Phase 4: 100% traffic              â”‚
â”‚ â””â”€ Complete migration              â”‚
â”‚                                    â”‚
â”‚ At any point:                      â”‚
â”‚ â””â”€ If metrics degrade â†’ ROLLBACK   â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Traffic Split Timeline:**

```
% Traffic â†’
100â”‚                          â–ˆâ–ˆâ–ˆâ–ˆ
   â”‚                      â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘
   â”‚                  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
 50â”‚              â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
   â”‚          â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
   â”‚      â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
  0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Time
      0m   10m  20m  30m  40m

    â–‘ Old version
    â–ˆ New version (canary)
```

**Automated Canary with Flagger:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Flagger Canary Analysis           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚  Metrics checked:                    â”‚
â”‚  â”œâ”€ Request success rate > 99%       â”‚
â”‚  â”œâ”€ Request duration p99 < 500ms     â”‚
â”‚  â””â”€ Error rate < 1%                  â”‚
â”‚                                      â”‚
â”‚  Analysis window: 1 minute           â”‚
â”‚  Iterations: 10                      â”‚
â”‚  Step weight: 10%                    â”‚
â”‚                                      â”‚
â”‚  If 5 consecutive failures:          â”‚
â”‚  â””â”€ Automatic rollback               â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Feature Flags

DÃ©ployer le code mais contrÃ´ler l'activation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Feature Flag Rollout            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Deploy v2 with new feature (flag OFF)
    â”‚
    â–¼
Enable for 1% of users (internal)
    â”‚
    â”œâ”€ Monitor metrics
    â”œâ”€ Gather feedback
    â”‚
    â–¼
Enable for 10% of users
    â”‚
    â”œâ”€ Monitor
    â”‚
    â–¼
Enable for 50% of users
    â”‚
    â”œâ”€ Monitor
    â”‚
    â–¼
Enable for 100% of users
    â”‚
    â–¼
Remove flag (cleanup)

Rollback = Just flip flag OFF
(No redeployment needed)
```

**Flag Types:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Release Flags                 â”‚
â”‚  â””â”€ Short-lived, removed       â”‚
â”‚     after full rollout         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Ops Flags                     â”‚
â”‚  â””â”€ Long-lived, control        â”‚
â”‚     system behavior            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Experiment Flags              â”‚
â”‚  â””â”€ A/B testing                â”‚
â”‚     Temporary                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparison Table

| Strategy      | Speed  | Risk  | Rollback | Resource | Use Case        |
|---------------|--------|-------|----------|----------|-----------------|
| Rolling       | Medium | Low   | Slow     | Low      | Standard        |
| Blue-Green    | Instant| Low   | Instant  | High     | Critical apps   |
| Canary        | Slow   | V.Low | Fast     | Medium   | High-risk       |
| Feature Flags | Instant| V.Low | Instant  | Low      | Experimentation |

## Practical Patterns

### Health Checks

Signaler l'Ã©tat de l'application Ã  Kubernetes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Kubernetes Probes              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Liveness Probe:
â”œâ”€ Question: "Is the app alive?"
â”œâ”€ If fails: Kill & restart pod
â”œâ”€ Example: /healthz
â””â”€ Check: Basic app responsiveness

Readiness Probe:
â”œâ”€ Question: "Can it handle traffic?"
â”œâ”€ If fails: Remove from service endpoints
â”œâ”€ Example: /ready
â””â”€ Check: Dependencies (DB, cache, etc.)

Startup Probe:
â”œâ”€ Question: "Has it started?"
â”œâ”€ If fails: Kill & restart (slow start apps)
â”œâ”€ Example: /startup
â””â”€ Check: Initial startup complete
```

**Probe Flow:**

```
Pod Starting
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Startup Probe â”‚ â†’ Fail â†’ Kill pod
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Pass
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Liveness      â”‚ â†’ Fail â†’ Restart pod
â”‚(continuous)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Pass
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Readiness     â”‚ â†’ Fail â†’ Remove from endpoints
â”‚(continuous)  â”‚           (but don't kill)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Pass
       â–¼
   Receive traffic
```

**Health Check Logic:**

```
Liveness (/healthz):
â””â”€ Return 200 OK
   Simple, fast check

Readiness (/ready):
â”œâ”€ Check database connection
â”œâ”€ Check cache connection  
â”œâ”€ Check external APIs
â””â”€ If any fails â†’ 503

Don't:
âœ— Use liveness for dependency checks
  (causes cascading restarts)
âœ— Make health checks too slow (>1s)
âœ— Check external services in liveness
```

### Graceful Shutdown

ArrÃªter proprement sans perdre de requÃªtes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Shutdown Sequence               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SIGTERM received
    â”‚
    â–¼
1. Stop accepting new requests
   â””â”€ Return 503 to load balancer
    â”‚
    â–¼
2. Wait for in-flight requests
   â””â”€ Timeout: 30s
    â”‚
    â–¼
3. Close connections
   â””â”€ DB, cache, queues
    â”‚
    â–¼
4. Flush logs/metrics
    â”‚
    â–¼
5. Exit

If > 30s: SIGKILL (forceful)
```

**Kubernetes Lifecycle:**

```
Time â†’
0s      5s     15s    30s    35s

â”‚       â”‚      â”‚      â”‚      â”‚
â”‚       â”‚      â”‚      â”‚      â”‚
â–¼       â–¼      â–¼      â–¼      â–¼
Deploy  SIGTERM Drain Grace  SIGKILL
        sent    done  period (force)
                      ends

terminationGracePeriodSeconds: 30s
preStop hook: /shutdown
```

### Resource Limits

ContrÃ´ler l'utilisation des ressources.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Resource Management             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚
â”‚  Requests (Scheduler garantit):   â”‚
â”‚  â”œâ”€ CPU: 100m (0.1 core)          â”‚
â”‚  â””â”€ Memory: 128Mi                  â”‚
â”‚                                    â”‚
â”‚  Limits (Hard cap):                â”‚
â”‚  â”œâ”€ CPU: 500m (0.5 core)          â”‚
â”‚  â”‚  â””â”€ Throttled si dÃ©passÃ©       â”‚
â”‚  â”‚                                 â”‚
â”‚  â””â”€ Memory: 512Mi                  â”‚
â”‚     â””â”€ OOMKilled si dÃ©passÃ©       â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**QoS Classes:**

```
Guaranteed (Best):
â”œâ”€ requests = limits
â”œâ”€ Lowest eviction priority
â””â”€ Use for critical services

Burstable (Medium):
â”œâ”€ requests < limits
â”œâ”€ Medium eviction priority
â””â”€ Use for most apps

BestEffort (Worst):
â”œâ”€ No requests or limits
â”œâ”€ First to be evicted
â””â”€ Use for non-critical batch jobs
```

**Resource Patterns:**

```
CPU-bound app:
requests: 
  cpu: 1000m
  memory: 256Mi
limits:
  cpu: 2000m      â† Can burst
  memory: 256Mi   â† Fixed

Memory-bound app:
requests:
  cpu: 100m
  memory: 2Gi
limits:
  cpu: 500m       â† Can burst
  memory: 2Gi     â† Fixed (avoid OOM)
```

### Autoscaling

Adapter automatiquement les ressources.

**HPA (Horizontal Pod Autoscaler):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    HPA Scaling Logic               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Current CPU: 80%
Target CPU: 50%
Current pods: 3

Desired pods = ceil(3 Ã— 80/50) = 5

Scale up to 5 pods
    â”‚
    â–¼
CPU drops to 40% (below target)
    â”‚
    â–¼
Wait stabilization (5min)
    â”‚
    â–¼
Desired pods = ceil(5 Ã— 40/50) = 4

Scale down to 4 pods
```

**Scaling Behavior:**

```
Scale Up:
â”œâ”€ Fast (default: double every 15s)
â”œâ”€ No stabilization window
â””â”€ React quickly to load spikes

Scale Down:
â”œâ”€ Slow (default: 1 pod/5min)
â”œâ”€ Stabilization window: 5min
â””â”€ Prevent flapping
```

**VPA (Vertical Pod Autoscaler):**

```
Monitors resource usage
    â”‚
    â–¼
Recommends new requests/limits
    â”‚
    â–¼
Can auto-update pods
(requires restart)

Use for:
â””â”€ Apps with variable resource needs
   that can't scale horizontally
```

**KEDA (Event-driven Autoscaling):**

```
Scalers:
â”œâ”€ Kafka queue depth
â”œâ”€ RabbitMQ messages
â”œâ”€ Prometheus metrics
â”œâ”€ Cron schedules
â””â”€ HTTP requests

Example:
Queue depth > 100 â†’ Scale to 10 pods
Queue depth < 10 â†’ Scale to 1 pod
Queue empty for 5min â†’ Scale to 0
```

## Resiliency Maturity Model

Progression vers une architecture rÃ©siliente.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Level 0: Reactive                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ No monitoring                            â”‚
â”‚ â€¢ Manual deployments                       â”‚
â”‚ â€¢ No health checks                         â”‚
â”‚ â€¢ Learn from production outages            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Level 1: Aware                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Basic metrics (CPU, memory)              â”‚
â”‚ â€¢ Automated deployments                    â”‚
â”‚ â€¢ Liveness/readiness probes                â”‚
â”‚ â€¢ Some logging                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Level 2: Proactive                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Golden signals monitored                 â”‚
â”‚ â€¢ SLOs defined                             â”‚
â”‚ â€¢ Structured logging                       â”‚
â”‚ â€¢ Alerting on SLO burn rate                â”‚
â”‚ â€¢ Circuit breakers                         â”‚
â”‚ â€¢ Retry policies                           â”‚
â”‚ â€¢ Rolling updates                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Level 3: Resilient                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Distributed tracing                      â”‚
â”‚ â€¢ Error budget tracking                    â”‚
â”‚ â€¢ Rate limiting                            â”‚
â”‚ â€¢ Graceful degradation                     â”‚
â”‚ â€¢ Canary deployments                       â”‚
â”‚ â€¢ Chaos engineering (staging)              â”‚
â”‚ â€¢ Runbooks for common incidents            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Level 4: Antifragile               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Production chaos engineering             â”‚
â”‚ â€¢ Automatic remediation                    â”‚
â”‚ â€¢ AI-driven anomaly detection              â”‚
â”‚ â€¢ Self-healing systems                     â”‚
â”‚ â€¢ Game days culture                        â”‚
â”‚ â€¢ Continuous improvement from incidents    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Incident Response Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Incident Lifecycle                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DETECTION
   â”œâ”€ Alert fires (burn rate, error spike)
   â”œâ”€ On-call engineer notified
   â””â”€ Severity assessed
       â”‚
       â–¼
2. TRIAGE
   â”œâ”€ Check dashboards (metrics, logs, traces)
   â”œâ”€ Identify affected services
   â”œâ”€ Determine scope (users impacted)
   â””â”€ Page additional help if needed
       â”‚
       â–¼
3. MITIGATION
   â”œâ”€ Implement quick fix (rollback, scale, etc.)
   â”œâ”€ NOT root cause fix (yet)
   â””â”€ Goal: Restore service ASAP
       â”‚
       â–¼
4. COMMUNICATION
   â”œâ”€ Update status page
   â”œâ”€ Notify stakeholders
   â””â”€ Regular updates every 30min
       â”‚
       â–¼
5. RESOLUTION
   â”œâ”€ Service restored
   â”œâ”€ Monitoring stabilization
   â””â”€ All-clear declared
       â”‚
       â–¼
6. POST-MORTEM (within 48h)
   â”œâ”€ Timeline of events
   â”œâ”€ Root cause analysis
   â”œâ”€ What went well
   â”œâ”€ What went wrong
   â””â”€ Action items (with owners)
       â”‚
       â–¼
7. FOLLOW-UP
   â”œâ”€ Implement action items
   â”œâ”€ Update runbooks
   â””â”€ Share learnings team-wide
```

## Checklist

### Pre-Production

```
Observability:
â˜ Golden signals dashboards
â˜ Distributed tracing configured
â˜ Structured logging implemented
â˜ Alerts on SLO burn rate
â˜ Runbooks for common scenarios

Resilience Patterns:
â˜ Circuit breakers on external deps
â˜ Retry logic with exp backoff
â˜ Rate limiting configured
â˜ Timeouts on all external calls
â˜ Graceful degradation plan

Kubernetes Config:
â˜ Liveness probe configured
â˜ Readiness probe configured
â˜ Resource requests/limits set
â˜ HPA configured
â˜ PodDisruptionBudget defined

Deployment:
â˜ Canary/Blue-Green strategy
â˜ Rollback plan documented
â˜ Feature flags for risky changes
â˜ Database migrations backward-compatible

SLOs:
â˜ SLOs defined and documented
â˜ Error budget calculated
â˜ Stakeholders aligned on targets
```

### Continuous Operations

```
â˜ Weekly SLO review
â˜ Monthly error budget review
â˜ Quarterly game day
â˜ Post-mortem for all P1 incidents
â˜ Regular chaos experiments
â˜ Dashboard accuracy checks
â˜ Alert fatigue assessment
â˜ Runbook updates after incidents
```

### Post-Incident

```
â˜ Timeline documented
â˜ Root cause identified
â˜ Impact quantified
â˜ Action items created
â˜ Runbook updated
â˜ Team debrief completed
â˜ Learnings shared
```

## Resources

### Books
- **Site Reliability Engineering** (Google)
    - Bible du SRE, SLOs, error budgets
- **The DevOps Handbook**
    - Culture, pratiques, Ã©tudes de cas
- **Release It!** (Michael Nygard)
    - Patterns de rÃ©silience
- **Chaos Engineering** (Netflix)
    - Guide pratique du chaos

### Tools Ecosystem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Observability Stack         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metrics:  Prometheus, Grafana       â”‚
â”‚ Logs:     Loki, Fluentd             â”‚
â”‚ Traces:   Jaeger, Tempo, Zipkin     â”‚
â”‚ APM:      OpenTelemetry             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Chaos Engineering           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ K8s:      Chaos Mesh, Litmus        â”‚
â”‚ Network:  Toxiproxy                 â”‚
â”‚ AWS:      AWS FIS                   â”‚
â”‚ GCP:      Chaos Toolkit             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Progressive Delivery        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Canary:   Flagger, Argo Rollouts    â”‚
â”‚ Flags:    Unleash, LaunchDarkly     â”‚
â”‚ Traffic:  Istio, Linkerd            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Policy & Governance         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Policy:   Kyverno, OPA/Gatekeeper   â”‚
â”‚ Security: Falco, Trivy              â”‚
â”‚ Cost:     Kubecost                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Standards
- **OpenTelemetry** - Observability standard
- **OpenMetrics** - Metrics exposition
- **CloudEvents** - Event format
- **CNCF Landscape** - Ecosystem map

### Learning Resources
- **SRE Book** (free): sre.google/books
- **OpenTelemetry Docs**: opentelemetry.io
- **Chaos Mesh Docs**: chaos-mesh.org
- **Kyverno Docs**: kyverno.io
- **CNCF YouTube**: Cloud native patterns & talks

---

**Remember:**

> "Hope is not a strategy. Design for failure."

Les systÃ¨mes rÃ©silients ne sont pas ceux qui ne tombent jamais en panne, mais ceux qui savent comment gÃ©rer les pannes quand elles arrivent.
